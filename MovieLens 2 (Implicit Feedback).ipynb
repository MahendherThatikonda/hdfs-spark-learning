{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Lens Implicit Feedback ###\n",
    "\n",
    "The movie lens dataset contains a variety of separate datasets, including movie metadata, user-movie ratings, user-movie tag applications, and tag metadata (relevance scores). This allows us to explore a variety of models, including regression, classification, and ranking.\n",
    "\n",
    "This example uses the user-movie ratings to fit an implicit feedback model that we could use to recommend movies to users based on the implied preference which is based on their rating.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "- [Data](#Data)\n",
    "- [Implicit feedback](#Implicit-feedback)\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The `randomSplit` method will return a different random split every time the output is used unless the random seed is specified or the output is cached.\n",
    "- We are predicting implicit preference so we can't use RMSE to evaluate the performance of the model, we can only generate recommendations and check if those recommendations match any of the relevant items in the test data that was kept from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=2, worker_memory=4, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports and code here or insert cells below\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ideal number of partitions\n",
    "\n",
    "conf = sc.getConf()\n",
    "\n",
    "N = int(conf.get(\"spark.executor.instances\"))\n",
    "M = int(conf.get(\"spark.executor.cores\"))\n",
    "partitions = 4 * N * M\n",
    "\n",
    "print(f'ideal # partitions = {partitions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ###\n",
    "\n",
    "The user-movie rating data is stored in a sparse format with one rating per row.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- Each row corresponds to an observed rating, and there will be many more user-movie ratings that are unobserved.\n",
    "- The user and movie identifiers are already increasing integers otherwise we would need to use the `StringIndexer` class to convert them into increasing integers as that is what is required by `ALS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from HDFS\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('movie_id', IntegerType(), True),\n",
    "    StructField('rating', FloatType(), True),\n",
    "    StructField('timestamp', LongType(), True),\n",
    "])\n",
    "\n",
    "data = (\n",
    "    spark.read.csv(\"hdfs:///data/ml/ratings.csv\", header=True, inferSchema=False, schema=schema)\n",
    "    .repartition(partitions)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "data.printSchema()\n",
    "show_as_html(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit feedback ###\n",
    "\n",
    "We will use the Alternating Least Squares class `ALS` for implicit feedback.\n",
    "\n",
    "This is a specific optimisation algorithm that solves the low rank matrix factorization problem underlying the collaborative filtering model.\n",
    "\n",
    "We will evaluate the following ranking metrics,\n",
    "\n",
    "- Precision\n",
    "- Mean Average Precision (MAP)\n",
    "- NDCG\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The `ALS` class expects the user and item identifiers to be encoded as increasing integers starting at 0.\n",
    "- We are predicting implicit preference so we can't use RMSE to evaluate the performance of the model, we can only generate recommendations and check if those recommendations match any of the relevant items in the test data that was kept from the model.\n",
    "- Note that these offline metrics are not ideal as the test data only contains a small sample of relevant items and the recommendations that are not in the test data may still be interesting to the user.\n",
    "- The best way to evaluate a recommendation system is to test it on a subset of real users in the real world and to see how they interact with the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training\n",
    "\n",
    "training, test = data.randomSplit([0.7, 0.3])\n",
    "training.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternating least squares (implicitPrefs=False)\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"movie_id\", ratingCol=\"rating\", implicitPrefs=True)\n",
    "alsModel = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relevance scores (not necessary if we just want to generate recommendations)\n",
    "\n",
    "pred = alsModel.transform(test)\n",
    "pred.cache()\n",
    "\n",
    "pred.printSchema()\n",
    "show_as_html(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "\n",
    "K = 5\n",
    "\n",
    "recommendations = alsModel.recommendForAllUsers(K)\n",
    "\n",
    "recommendations.printSchema()\n",
    "show_as_html(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and sort relevant items from test in order of descending relevance (e.g. rating)\n",
    "\n",
    "relevant = (\n",
    "    test\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.reverse(F.sort_array(F.collect_list(\n",
    "            F.struct(\n",
    "                F.col(\"rating\"),\n",
    "                F.col(\"movie_id\"),\n",
    "            )\n",
    "        ))).alias(\"relevant\")\n",
    "    )\n",
    ")\n",
    "relevant.printSchema()\n",
    "show_as_html(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge recommendations and relevant items so they can be compared\n",
    "\n",
    "temp = (\n",
    "    recommendations.select(\n",
    "        F.col(\"user_id\"),\n",
    "        F.col(\"recommendations.movie_id\").alias(\"recommendations\").astype(ArrayType(DoubleType())),\n",
    "    )\n",
    "    .join(\n",
    "        relevant.select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.col(\"relevant.movie_id\").alias(\"relevant\").astype(ArrayType(DoubleType())),\n",
    "        ),\n",
    "        on=\"user_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "temp.cache()\n",
    "\n",
    "temp.printSchema()\n",
    "show_as_html(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "K = 5\n",
    "\n",
    "evaluator = RankingEvaluator(predictionCol=\"recommendations\", labelCol=\"relevant\")\n",
    "\n",
    "precisionAtK = evaluator.evaluate(temp, {evaluator.metricName: \"precisionAtK\",            evaluator.k: K})\n",
    "mapAtK       = evaluator.evaluate(temp, {evaluator.metricName: \"meanAveragePrecisionAtK\", evaluator.k: K})\n",
    "ndcgAtK      = evaluator.evaluate(temp, {evaluator.metricName: \"ndcgAtK\",                 evaluator.k: K})\n",
    "\n",
    "print(f'metrics for implicit feedback')\n",
    "print(f'')\n",
    "print(f'precision @ K: {precisionAtK:.5f}')\n",
    "print(f'MAP @ K:       {mapAtK:.5f}')\n",
    "print(f'NDCG @ K:      {ndcgAtK:.5f}')\n",
    "print(f'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
